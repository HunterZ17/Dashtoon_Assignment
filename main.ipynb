{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerNetwork, self).__init__()\n",
    "        self.ConvBlock = nn.Sequential(\n",
    "            ConvLayer(3, 32, 9, 1),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(32, 64, 3, 2),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(64, 128, 3, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ResidualBlock = nn.Sequential(\n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3)\n",
    "        )\n",
    "        self.DeconvBlock = nn.Sequential(\n",
    "            DeconvLayer(128, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            DeconvLayer(64, 32, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(32, 3, 9, 1, norm=\"None\")\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ConvBlock(x)\n",
    "        x = self.ResidualBlock(x)\n",
    "        out = self.DeconvBlock(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, norm=\"instance\"):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        # Padding Layers\n",
    "        padding_size = kernel_size // 2\n",
    "        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n",
    "\n",
    "        # Convolution Layer\n",
    "        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.norm_type = norm\n",
    "        if (norm==\"instance\"):\n",
    "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
    "        elif (norm==\"batch\"):\n",
    "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reflection_pad(x)\n",
    "        x = self.conv_layer(x)\n",
    "        if (self.norm_type==\"None\"):\n",
    "            out = x\n",
    "        else:\n",
    "            out = self.norm_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, channels=128, kernel_size=3):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x                     # preserve residual\n",
    "        out = self.relu(self.conv1(x))   # 1st conv layer + activation\n",
    "        out = self.conv2(out)            # 2nd conv layer\n",
    "        out = out + identity             # add residual\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding, norm=\"instance\"):\n",
    "        super(DeconvLayer, self).__init__()\n",
    "\n",
    "        # Transposed Convolution \n",
    "        padding_size = kernel_size // 2\n",
    "        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding_size, output_padding)\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.norm_type = norm\n",
    "        if (norm==\"instance\"):\n",
    "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
    "        elif (norm==\"batch\"):\n",
    "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_transpose(x)\n",
    "        if (self.norm_type==\"None\"):\n",
    "            out = x\n",
    "        else:\n",
    "            out = self.norm_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerNetwork(\n",
      "  (ConvBlock): Sequential(\n",
      "    (0): ConvLayer(\n",
      "      (reflection_pad): ReflectionPad2d((4, 4, 4, 4))\n",
      "      (conv_layer): Conv2d(3, 32, kernel_size=(9, 9), stride=(1, 1))\n",
      "      (norm_layer): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): ConvLayer(\n",
      "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv_layer): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (norm_layer): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): ConvLayer(\n",
      "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv_layer): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (norm_layer): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (ResidualBlock): Sequential(\n",
      "    (0): ResidualLayer(\n",
      "      (conv1): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (norm_layer): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (conv2): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (norm_layer): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualLayer(\n",
      "      (conv1): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (norm_layer): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (conv2): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (norm_layer): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualLayer(\n",
      "      (conv1): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (norm_layer): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (conv2): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (norm_layer): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualLayer(\n",
      "      (conv1): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (norm_layer): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (conv2): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (norm_layer): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualLayer(\n",
      "      (conv1): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (norm_layer): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (conv2): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv_layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (norm_layer): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (DeconvBlock): Sequential(\n",
      "    (0): DeconvLayer(\n",
      "      (conv_transpose): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (norm_layer): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): DeconvLayer(\n",
      "      (conv_transpose): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (norm_layer): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): ConvLayer(\n",
      "      (reflection_pad): ReflectionPad2d((4, 4, 4, 4))\n",
      "      (conv_layer): Conv2d(32, 3, kernel_size=(9, 9), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "TransformerNetwork = TransformerNetwork().to(device)\n",
    "print(TransformerNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lokes\\Desktop\\CODES\\Dashtoon_Assignment\\main.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokes/Desktop/CODES/Dashtoon_Assignment/main.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokes/Desktop/CODES/Dashtoon_Assignment/main.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lokes/Desktop/CODES/Dashtoon_Assignment/main.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokes/Desktop/CODES/Dashtoon_Assignment/main.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokes/Desktop/CODES/Dashtoon_Assignment/main.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m plt\u001b[39m.\u001b[39mrcParams[\u001b[39m\"\u001b[39m\u001b[39mfigure.figsize\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m20\u001b[39m,\u001b[39m10\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import os\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lokes\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lokes\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        # The first number x in convx_y gets added by 1 after it has gone\n",
    "        # through a maxpool, and the second y if we have several conv layers\n",
    "        # in between a max pool. These strings (0, 5, 10, ..) then correspond\n",
    "        # to conv1_1, conv2_1, conv3_1, conv4_1, conv5_1 mentioned in NST paper\n",
    "        self.chosen_features = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n",
    "\n",
    "        # We don't need to run anything further than conv5_1 (the 28th module in vgg)\n",
    "        # Since remember, we dont actually care about the output of VGG: the only thing\n",
    "        # that is modified is the generated image (i.e, the input).\n",
    "        self.model = models.vgg19(pretrained=True).features[:29]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store relevant features\n",
    "        features = []\n",
    "\n",
    "        # Go through each layer in model, if the layer is in the chosen_features,\n",
    "        # store it in features. At the end we'll just return all the activations\n",
    "        # for the specific layers we have in chosen_features\n",
    "        for layer_num, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "\n",
    "            if str(layer_num) in self.chosen_features:\n",
    "                features.append(x)\n",
    "\n",
    "        return features\n",
    "model = VGG().to(device).eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 256\n",
    "loader = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((imsize, imsize)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x  )\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "def load_image(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    #print(image.shape)\n",
    "    \n",
    "    return image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scandir: path should be string, bytes, os.PathLike or None, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lokes\\Desktop\\CODES\\Dashtoon_Assignment\\main.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokes/Desktop/CODES/Dashtoon_Assignment/main.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m STYLE_WEIGHT \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokes/Desktop/CODES/Dashtoon_Assignment/main.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m TV_WEIGHT \u001b[39m=\u001b[39m \u001b[39m1e-6\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lokes/Desktop/CODES/Dashtoon_Assignment/main.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mImageFolder(dataset, transform\u001b[39m=\u001b[39;49mloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokes/Desktop/CODES/Dashtoon_Assignment/main.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    310\u001b[0m         root,\n\u001b[0;32m    311\u001b[0m         loader,\n\u001b[0;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    313\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[0;32m    314\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[0;32m    315\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[0;32m    316\u001b[0m     )\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\datasets\\folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    135\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    136\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    142\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 144\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[0;32m    145\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\datasets\\folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[0;32m    192\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \n\u001b[0;32m    194\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\datasets\\folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[0;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[39m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[0;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[0;32m     42\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: scandir: path should be string, bytes, os.PathLike or None, not Tensor"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "dataset = load_image(\"fub.jpeg\")\n",
    "style_name=load_image(\"style.jpeg\")\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "total_steps = 12000\n",
    "learning_rate = 0.0001\n",
    "BATCH_SIZE = 1 \n",
    "CONTENT_WEIGHT = 17\n",
    "STYLE_WEIGHT = 50\n",
    "TV_WEIGHT = 1e-6\n",
    "\n",
    "train_dataset = datasets.ImageFolder(dataset, transform=loader)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotT(tensor):\n",
    "    # Add the means\n",
    "    #ttoi_t = transforms.Compose([\n",
    "    #    transforms.Normalize([-103.939, -116.779, -123.68],[1,1,1])])\n",
    "\n",
    "    # Remove the batch_size dimension\n",
    "    tensor = tensor.squeeze()\n",
    "    #img = ttoi_t(tensor)\n",
    "    img = tensor.cpu().numpy()\n",
    "    \n",
    "    # Transpose from [C, H, W] -> [H, W, C]\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    img = np.array(img).clip(0,1)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
